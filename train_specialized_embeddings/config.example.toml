# Example configuration file for triplet loss training
# Copy this file and modify as needed, or use environment variables with TRIPLET_TRAINING_ prefix

[triplet_training]

# Data paths
train_parquet_path = "data/merged/train.parquet"
val_parquet_path = "data/merged/val.parquet"
difficulty_metadata_path = "data/difficulty/difficulty_metadata.parquet"
checkpoint_dir = "checkpoints/triplet"
output_embeddings_dir = "data/specialized_embeddings"  # Optional

# Model architecture
input_dim = 768          # DINOv2 base embedding dimension
hidden_dim = 512         # Hidden layer dimension
output_dim = 256         # Output embedding dimension
dropout = 0.1            # Dropout probability
use_residual = true      # Use residual shortcut connection
use_layer_norm = true    # Use LayerNorm after each hidden layer

# Training hyperparameters
batch_size = 256         # Batch size for training
num_epochs = 50          # Number of training epochs
learning_rate = 0.0001   # Learning rate (1e-4)
weight_decay = 0.00001   # Weight decay for optimizer (1e-5)
margin = 0.5             # Triplet loss margin
loss_distance = "euclidean"  # Distance metric: "euclidean" or "cosine"
samples_per_epoch = 250000   # Maximum triplet samples drawn per epoch

# UCB sampler configuration
ucb_exploration_constant = 2.0    # UCB exploration constant (c)
ucb_warmup_samples = 1000        # Number of warmup samples before UCB kicks in

# Training configuration
device = "auto"          # Device: "cuda", "cpu", or "auto"
num_workers = 4          # Number of data loader workers
pin_memory = true        # Pin memory for data loader
seed = 42                # Random seed for reproducibility

# Checkpointing and validation
save_every_n_epochs = 5      # Save checkpoint every N epochs
validate_every_n_epochs = 1  # Run validation every N epochs
resume_from_checkpoint = null  # Path to checkpoint to resume from (optional)

# Logging
log_file = null          # Optional log file path
log_every_n_batches = 100  # Log metrics every N batches

