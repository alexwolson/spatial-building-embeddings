# Unified configuration file for the spatial building embeddings pipeline
# This is the actual configuration file for this project.
#
# This file is organized by type of configuration rather than by workflow:
# - [global] section: Shared settings used across all workflows
# - [paths] section: All pipeline file and directory paths
# - [embedding_model] section: Model settings for embedding generation
# - [training_model] section: Model architecture settings for training
# - [training] section: Training hyperparameters and settings
# - [data] section: Data processing settings (splits, neighbors, etc.)
# - [infrastructure] section: Device, workers, and performance settings
# - [logging] section: Logging and monitoring settings
#
# Workflows automatically read from the relevant sections based on their needs.

[global]
# Random seed for reproducibility (used by workflows that don't specify their own seed)
seed = 42

# Base directory for all log files (mandatory)
# All workflows will create log files within this directory
log_dir = "/home/awolson/scratch/spatial-building-embeddings/logs"

[paths]
# Pipeline directory paths (shared across workflows)
intermediates_dir = "/home/awolson/scratch/spatial-building-embeddings/intermediates"      # Output of process_tar, input to merge_and_split
embeddings_dir = "/home/awolson/scratch/spatial-building-embeddings/embeddings"            # Output of generate_embeddings, input to merge_and_split
merged_dir = "/home/awolson/scratch/spatial-building-embeddings/merged"                    # Output of merge_and_split, input to difficulty_metadata and triplet_training

# Training data paths
train_parquet_path = "/home/awolson/scratch/spatial-building-embeddings/merged/train.parquet"
val_parquet_path = "/home/awolson/scratch/spatial-building-embeddings/merged/val.parquet"
difficulty_metadata_path = "/home/awolson/scratch/spatial-building-embeddings/difficulty/difficulty_metadata.parquet"  # Output of difficulty_metadata, input to triplet_training

# Training output paths
checkpoint_dir = "/home/awolson/scratch/spatial-building-embeddings/checkpoints/triplet"

[embedding_model]
# Model settings for embedding generation (generate_embeddings workflow)
model_name = "nomic-ai/nomic-embed-vision-v1.5"
batch_size = 128  # Batch size for inference (default: 128 for H100 80GB GPUs on Nibi)

[training_model]
input_dim = 768          # nomic-embed-vision-v1.5 embedding dimension (fixed)
output_dim = 256         # Output embedding dimension (fixed)

[training]
num_epochs = 50         # Number of training epochs

# Checkpointing and validation
save_every_n_epochs = 5      # Save checkpoint every N epochs
validate_every_n_epochs = 1  # Run validation every N epochs
resume_from_checkpoint = null  # Path to checkpoint to resume from (optional)
early_stopping_patience = 3  # Validations with no improvement before stopping (0 disables)

# Retrieval metrics
retrieval_metric_top_k = [1, 5, 10, 100, 1000]
retrieval_metric_max_queries = 4096
retrieval_metric_per_building_limit = 4

# Best training configuration (for fetch_best_and_train.py)
best_training_trial_name_pattern = ".*-trial-.*"  # Regex pattern to match Optuna trial run names in WandB
best_training_metric_name = "val/retrieval_recall@100"  # Metric name to use for selecting best run from WandB

[data]
# Data processing settings
# Split ratios (must sum to 1.0)
train_ratio = 0.7
val_ratio = 0.15
test_ratio = 0.15

# Difficulty metadata parameters
neighbors = 150                    # Number of neighbours to retain per anchor
k0_for_local_scale = 50           # Neighbour rank used to define the local scale L(a)
sample_fraction_for_bands = 0.03  # Fraction of anchors sampled to calibrate band edges

[infrastructure]
# Device and performance settings
# device, num_workers, and pin_memory are determined dynamically in the code

# Difficulty metadata performance settings
distance_dtype = "float32"  # Floating point precision: "float32" or "float64"
batch_size = 100000         # Number of anchors to process per BallTree query batch (for difficulty_metadata)
row_group_size = 50000      # Row group size for the output parquet writer

[logging]
# Logging and monitoring settings
log_every_n_batches = 100  # Log metrics every N batches (for training)

# Weights & Biases configuration
wandb_enabled = true        # Enable Weights & Biases logging
wandb_project = "spatial-building-embeddings-vlm"  # wandb project name
wandb_run_name = null       # Optional explicit run name (null = auto-generate)
wandb_mode = "online"       # Use "online" to sync immediately, "offline" to defer
